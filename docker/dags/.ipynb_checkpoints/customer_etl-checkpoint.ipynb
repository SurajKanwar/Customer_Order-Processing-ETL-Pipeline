{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f5e97-6800-48a4-b535-50eef0ddc4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.decorators import task\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.utils.dates import days_ago\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# File path to the local CSV file\n",
    "CSV_FILE_PATH = r'C:\\Users\\Dusty\\Downloads\\Internship\\Git_Repo\\Customer-Order-Processing-ETL-Pipeline\\data\\sample_orders.csv' \n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': days_ago(1),\n",
    "}\n",
    "\n",
    "with DAG(dag_id='customer_etl_pipeline',\n",
    "         default_args=default_args,\n",
    "         schedule_interval='@daily',\n",
    "         catchup=False) as dag:\n",
    "    \n",
    "    @task()\n",
    "    def extract_data():\n",
    "        \"\"\"Extract data from local CSV file instead of MySQL database.\"\"\"\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(CSV_FILE_PATH):\n",
    "            raise FileNotFoundError(f\"CSV file not found at {CSV_FILE_PATH}\")\n",
    "        \n",
    "        # Read data from the local CSV into a Pandas DataFrame\n",
    "        df_db = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "        # Convert DataFrame to JSON and return\n",
    "        return df_db.to_json()\n",
    "    \n",
    "    @task()\n",
    "    def transform_data(data):\n",
    "        \"\"\"Transform data: Cleaning and filtering.\"\"\"\n",
    "        df = pd.read_json(data)\n",
    "        \n",
    "        # Example transformation: remove rows with missing values\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # Example transformation: convert column names to lowercase\n",
    "        df.columns = df.columns.str.lower()\n",
    "\n",
    "        # Convert order_date to proper DATE format (if not already in the correct format)\n",
    "        df['order_date'] = pd.to_datetime(df['order_date']).dt.date\n",
    "        \n",
    "        return df.to_json()\n",
    "    \n",
    "    @task()\n",
    "    def create_table():\n",
    "        \"\"\"Create table in PostgreSQL if it doesn't exist.\"\"\"\n",
    "        connection = PostgresHook(postgres_conn_id='postgres_default').get_conn()\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS cleaned_sample_orders (\n",
    "            order_id INT PRIMARY KEY,\n",
    "            customer_name VARCHAR(50),\n",
    "            product_name VARCHAR(50),\n",
    "            quantity INT,\n",
    "            price INT,\n",
    "            order_date DATE\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(create_table_query)\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"Table checked/created successfully in PostgreSQL.\")\n",
    "    \n",
    "    @task()\n",
    "    def load_data(transformed_data):\n",
    "        \"\"\"Load transformed data into PostgreSQL database.\"\"\"\n",
    "        df = pd.read_json(transformed_data)\n",
    "\n",
    "        # Establish PostgreSQL connection\n",
    "        pg_hook = PostgresHook(postgres_conn_id='postgres_default')\n",
    "        connection = pg_hook.get_conn()\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Insert data row by row\n",
    "        for _, row in df.iterrows():\n",
    "            cursor.execute(\"\"\"\n",
    "            INSERT INTO cleaned_sample_orders (order_id, customer_name, product_name, quantity, price, order_date)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (order_id) DO NOTHING;  -- Prevents duplicate inserts\n",
    "            \"\"\", (\n",
    "                row['order_id'],\n",
    "                row['customer_name'],\n",
    "                row['product_name'],\n",
    "                row['quantity'],\n",
    "                row['price'],\n",
    "                row['order_date']\n",
    "            ))\n",
    "\n",
    "        # Commit and close connection\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        \n",
    "        print(\"Data loaded into PostgreSQL successfully.\")\n",
    "            \n",
    "    raw_data = extract_data()\n",
    "    transformed_data = transform_data(raw_data)\n",
    "    table_created = create_table()  # Ensure table exists\n",
    "    load_data_task = load_data(transformed_data)\n",
    "\n",
    "    # Set the correct order of execution\n",
    "    transformed_data >> table_created >> load_data_task\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
