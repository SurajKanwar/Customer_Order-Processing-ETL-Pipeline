## Project: Customer Order Processing ETL Pipeline

This repository contains the capstone project: **Customer Order Processing ETL Pipeline**. it will cover essential tools, libraries, and technologies required to build a robust data pipeline.

---

## Repository Structure:

```
root/
|-- README.md
|-- data/               # Contains sample datasets
|-- notebooks/          # Jupyter notebooks for training
|-- scripts/            # Python scripts for ETL tasks
|-- airflow_dags/       # Airflow DAGs for automation
|-- docker/             # Dockerfiles for containerization
|-- big_data/           # PySpark scripts
```

## Capstone Project: Customer Order Processing ETL Pipeline

**Objective:** Create an end-to-end ETL pipeline that processes customer order data, stores it in a database, and automates the workflow using Airflow.

#### **Steps:**
1. **Extract:** Read order data from a CSV file.
2. **Transform:** Clean data (e.g., handle missing values, format standardization).
3. **Load:** Store the cleaned data in a SQLite database.
4. **Automate:** Schedule the ETL pipeline with Airflow.
5. **Scale:** Use PySpark to process large datasets.
6. **Containerize:** Package the pipeline using Docker.


## References:

### Courses:
1. [Data Engineering on Google Cloud (Coursera)](https://www.coursera.org/professional-certificates/google-cloud-data-engineering)
2. [Introduction to SQL (Datacamp)](https://www.datacamp.com/courses/intro-to-sql-for-data-science)
3. [Big Data with PySpark (Coursera)](https://www.coursera.org/learn/big-data-analysis-with-spark)

### Tools Documentation:
- [Pandas Documentation](https://pandas.pydata.org/)
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Docker Documentation](https://docs.docker.com/)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)

---
